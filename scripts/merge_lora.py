#!/usr/bin/env python3
"""
åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹çš„è„šæœ¬
"""

import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def merge_lora_model():
    # é…ç½®è·¯å¾„
    # base_model_path = "Qwen/Qwen3-8B-Base"  # æˆ–ä½¿ç”¨æœ¬åœ°è·¯å¾„
    base_model_path = "/root/.cache/huggingface/hub/models--Qwen--Qwen3-8B-Base/snapshots/49e3418fbbbca6ecbdf9608b4d22e5a407081db4"
    
    lora_adapter_path = "/workspace/qwen3-8b-base-grpo-w-sft/qwen3-8b-base_gsm8k_simplerl_grpo_lora_8192/global_step_40/actor/lora_adapter"
    output_path = "/workspace/merged_qwen3_grpo_model"
    
    print("å¼€å§‹åˆå¹¶LoRAæ¨¡å‹...")
    print(f"åŸºç¡€æ¨¡å‹è·¯å¾„: {base_model_path}")
    print(f"LoRAé€‚é…å™¨è·¯å¾„: {lora_adapter_path}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    
    # é¦–å…ˆä¿®å¤adapter_config.jsonä¸­çš„base_model_name_or_path
    adapter_config_path = os.path.join(lora_adapter_path, "adapter_config.json")
    print(f"\nä¿®å¤adapteré…ç½®æ–‡ä»¶: {adapter_config_path}")
    
    with open(adapter_config_path, "r") as f:
        adapter_config = json.load(f)
    
    # æ›´æ–°base_model_name_or_pathä¸ºæ­£ç¡®çš„è·¯å¾„
    adapter_config["base_model_name_or_path"] = base_model_path
    
    with open(adapter_config_path, "w") as f:
        json.dump(adapter_config, f, indent=4)
    
    print("âœ“ å·²ä¿®å¤adapteré…ç½®æ–‡ä»¶")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"\nåŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    print("âœ“ åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½tokenizer
    print(f"åŠ è½½tokenizer: {base_model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True
    )
    print("âœ“ TokenizeråŠ è½½å®Œæˆ")
    
    # åŠ è½½LoRAé€‚é…å™¨
    print(f"\nåŠ è½½LoRAé€‚é…å™¨: {lora_adapter_path}")
    model_with_lora = PeftModel.from_pretrained(
        base_model,
        lora_adapter_path,
        torch_dtype=torch.bfloat16
    )
    print("âœ“ LoRAé€‚é…å™¨åŠ è½½å®Œæˆ")
    
    # åˆå¹¶æƒé‡
    print("\nåˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
    merged_model = model_with_lora.merge_and_unload()
    print("âœ“ æƒé‡åˆå¹¶å®Œæˆ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_path, exist_ok=True)
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"\nä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    merged_model.save_pretrained(
        output_path,
        torch_dtype=torch.bfloat16,
        safe_serialization=True
    )
    print("âœ“ æ¨¡å‹ä¿å­˜å®Œæˆ")
    
    # ä¿å­˜tokenizer
    print("ä¿å­˜tokenizer...")
    tokenizer.save_pretrained(output_path)
    print("âœ“ Tokenizerä¿å­˜å®Œæˆ")
    
    print(f"\nğŸ‰ åˆå¹¶å®Œæˆï¼åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åœ¨: {output_path}")
    print(f"ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è·¯å¾„è¿›è¡Œæ¨ç†:")
    print(f"model.path={output_path}")
    
    return output_path

if __name__ == "__main__":
    try:
        merged_path = merge_lora_model()
        
    except Exception as e:
        print(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()